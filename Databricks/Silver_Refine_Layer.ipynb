{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d0746b7-3950-40a7-a228-b7fcdc9ed0be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "#Regex to validate email and phone\n",
    "email_regex = r\"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$\"\n",
    "phone_regex = r\"^\\d{10}$\"  \n",
    "\n",
    "\n",
    "\n",
    "silver_df = spark.table(\"silver.guests\")  \n",
    "\n",
    "\n",
    "#Add Validation columns for email and phone\n",
    "silver_df = silver_df \\\n",
    "  .withColumn(\"email_valid\", F.col(\"email\").rlike(email_regex)) \\\n",
    "  .withColumn(\"phone_valid\",  F.regexp_replace(F.col(\"phone_number\"), \"[^0-9]\", \"\").rlike(phone_regex))\n",
    "\n",
    "\n",
    "\n",
    "silver_df = silver_df.withColumn(\n",
    "  \"data_valid\",\n",
    "  F.col(\"email_valid\") & F.col(\"phone_valid\")\n",
    ")\n",
    "\n",
    "#Check which validation failed\n",
    "silver_df = silver_df.withColumn(\n",
    "  \"comment\",\n",
    "  F.concat_ws(\" | \",\n",
    "    F.when(F.col(\"email_valid\"),  F.lit(\"email OK\"))\n",
    "     .otherwise(F.lit(\"email INVALID\")),\n",
    "    F.when(F.col(\"phone_valid\"),  F.lit(\"phone OK\"))\n",
    "     .otherwise(F.lit(\"phone INVALID\"))\n",
    "  )\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a31805e-ea93-4b01-ba4e-42df552aef1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save the dataframe as a Delta table \n",
    "tablename = \"guests_validated\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS silver.{tablename}\")\n",
    "\n",
    "silver_df.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"overwriteSchema\", \"true\") \\\n",
    "  .option(\"path\", f\"/mnt/silver/{tablename}/\") \\\n",
    "  .saveAsTable(f\"silver.{tablename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c4aa64-5b38-4dcf-9db8-f1f69ee73f11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "invalid_df = silver_df.filter(F.col(\"data_valid\") == False)\n",
    "\n",
    "\n",
    "invalid_df = silver_df.where(\"data_valid = false\")\n",
    "\n",
    "# Inspect\n",
    "invalid_df.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b7b073-958c-4d10-993c-88ad376f2717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tablename = \"guests_validated\"\n",
    "partition_column = \"state\"\n",
    "z_order_column = \"guest_number\"\n",
    "\n",
    "#Drop table if it exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS silver.{tablename}\")\n",
    "\n",
    "#Write partitioned Delta table\n",
    "silver_df.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"overwriteSchema\", \"true\") \\\n",
    "  .partitionBy(partition_column) \\\n",
    "  .option(\"path\", f\"/mnt/silver/{tablename}/\") \\\n",
    "  .saveAsTable(f\"silver.{tablename}\")\n",
    "\n",
    "\n",
    "spark.sql(f\"OPTIMIZE silver.{tablename} ZORDER BY ({z_order_column})\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8171888283690835,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Refine_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
